---
alwaysApply: true
---
# Test-Driven Development Rule

Enforce test-driven development practices with atomic steps and user verification.

<rule>
name: test_driven_development
description: Ensure all code changes follow TDD principles with incremental verification
filters:
  # Apply to all feature requests
  - type: content
    pattern: "(?i)(add|create|implement|build|fix|change|update)"
  # Apply to all code modifications
  - type: event
    pattern: "code_change"
  # Apply to function/method additions
  - type: content
    pattern: "(?i)(function|method|class|component)"
  # Auto-run tests on any source code changes
  - type: file_change
    pattern: "**/*.{py}"

actions:

- type: enforce
  message: |
  TEST-DRIVEN DEVELOPMENT PROTOCOL

  MANDATORY TDD WORKFLOW:

  1. **ALWAYS START WITH TESTS**:

     - Plan and write tests BEFORE writing implementation code
     - Create tests for every new feature, bug fix, or change
     - Follow Red-Green-Refactor cycle

  2. **ATOMIC TEST STEPS**:

     - Write ONE test at a time
     - Keep each test focused on a single behavior
     - Do not write long test suites all at once
     - Stop after each atomic test step

  3. **USER VERIFICATION REQUIRED**:

     - After writing each test, wait for user verification
     - Let user review and approve test before proceeding
     - Only continue to implementation after test approval
     - User must confirm each step before moving forward

  4. **BEHAVIOR-DRIVEN TESTING**:

     - Test USER BEHAVIOR and OUTCOMES, not implementation details
     - Focus on "Given X input, user should get Y result"
     - Test business logic and user workflows
     - Avoid testing internal methods, classes, or data structures
     - Tests should survive refactoring - they test WHAT, not HOW
     - Cover user scenarios, edge cases, and error conditions
     - Document any untested behaviors with clear reasoning

  5. **TEST FILE ORGANIZATION**:

     - Place test files next to their corresponding modules
     - Use `test_` prefix for test files (e.g., `calculator.py` → `test_calculator.py`)
     - Co-locate tests with source files for better maintainability
     - Follow Python testing conventions (pytest, unittest)
     - Use descriptive test function names starting with `test_`
     - Group related tests in classes when appropriate
     - Use pytest fixtures for setup/teardown when needed
     - Follow PEP 8 naming conventions for test functions
     - Adapt to existing test structure in codebase

  6. **INCREMENTAL APPROACH**:

     - Small, verifiable steps only
     - No large code blocks without tests
     - Each implementation should make tests pass
     - Refactor only after tests are green

  7. **CONTINUOUS TESTING**:
     - Any change to Python source files triggers all tests
     - Must ensure all tests pass before proceeding
     - Fix broken tests immediately
     - No code changes without verified test coverage
     - Use pytest or unittest for test execution

  WORKFLOW STEPS:

  1. Understand requirement
  2. Write failing test (Red)
  3. STOP - Get user verification
  4. Write minimal code to pass test (Green)
  5. RUN ALL TESTS - Ensure nothing breaks
  6. STOP - Get user verification
  7. Refactor if needed (Refactor)
  8. RUN ALL TESTS - Verify refactor didn't break anything
  9. STOP - Get user verification
  10. Repeat for next atomic feature

  AUTO-TEST TRIGGER:

  - Any Python file modification automatically runs the project's configured test command
  - **Test Command Detection Priority**:
    1. Look for `pytest.ini`, `pyproject.toml` with pytest config, or `conftest.py` → Use `pytest`
    2. Look for `test_*.py` files → Use `pytest` (if available) or `python -m unittest`
    3. Look for `*_test.py` files → Use `pytest` (if available) or `python -m unittest`
    4. Default fallback → `python -m unittest discover`
  - Must address any test failures before continuing development
  - Ensures continuous validation and early error detection

  PYTHON TESTING BEST PRACTICES:

  - Use pytest for modern Python testing (preferred over unittest)
  - Use `assert` statements for simple assertions
  - Use `pytest.raises()` for testing exceptions
  - Use `pytest.fixture` for test setup and teardown
  - Use `pytest.mark.parametrize` for parameterized tests
  - **Write test names that describe USER BEHAVIOR**: `test_user_can_add_two_numbers()` not `test_add_method()`
  - **Test public interfaces and user workflows**, not internal implementation
  - **Focus on outcomes**: "User gets correct result" not "Method returns value"
  - Use mocking (`unittest.mock` or `pytest-mock`) for external dependencies
  - **Avoid testing**: Private methods, internal state, implementation details
  - **Test scenarios**: "When user enters invalid data, they get helpful error message"

  BEHAVIOR-DRIVEN vs IMPLEMENTATION-DRIVEN TESTING:

  ❌ **AVOID Implementation Testing**:
  ```python
  def test_add_method():
      calc = Calculator()
      result = calc.add(2, 3)
      assert result == 5
  
  def test_calculator_has_add_method():
      calc = Calculator()
      assert hasattr(calc, 'add')
  ```

  ✅ **USE Behavior Testing**:
  ```python
  def test_user_can_add_numbers_and_get_correct_sum():
      # Given: User wants to add numbers
      calculator = Calculator()
      
      # When: User performs addition
      result = calculator.add(2, 3)
      
      # Then: User gets the expected result
      assert result == 5
  
  def test_user_gets_error_when_adding_invalid_input():
      # Given: User tries to add non-numeric values
      calculator = Calculator()
      
      # When: User attempts invalid operation
      # Then: User gets helpful error message
      with pytest.raises(ValueError, match="Invalid input"):
          calculator.add("abc", 3)
  ```

examples:

- input: |
  User: "Add a user authentication module"

  # Correct BEHAVIOR-DRIVEN TDD approach:

  Step 1: "I'll create `test_auth.py` and write a test that describes what a user should experience when authenticating - 'When a user provides valid credentials, they should be successfully logged in'..."
  
  ```python
  def test_user_can_login_with_valid_credentials():
      # Given: User has valid username and password
      username = "testuser"
      password = "validpass123"
      
      # When: User attempts to login
      result = authenticate_user(username, password)
      
      # Then: User should be successfully authenticated
      assert result.is_authenticated == True
      assert result.user_id is not None
  ```
  
  [Wait for user approval]
  Step 2: "Now I'll create `auth.py` with minimal code to make this behavior work..."
  [Wait for user approval]
  Step 3: "Let's add the next user scenario - what happens with invalid credentials..."

  output: "Following behavior-driven TDD focusing on user experience"

- input: |
  User: "Fix the bug in calculation"

  # Correct BEHAVIOR-DRIVEN TDD approach:

  Step 1: "I'll write a test that describes the expected user behavior when the bug is fixed - 'When a user calculates 2 + 3, they should get 5'..."
  
  ```python
  def test_user_gets_correct_calculation_result():
      # Given: User wants to calculate 2 + 3
      calculator = Calculator()
      
      # When: User performs the calculation
      result = calculator.calculate("2 + 3")
      
      # Then: User should get the correct result
      assert result == 5
  ```
  
  [Wait for user approval]
  Step 2: "Now I'll fix the code to make this user behavior work correctly..."

  output: "Bug fix following behavior-driven TDD principles"

- input: |
  User: "Create a Calculator class with add method"

  # Correct BEHAVIOR-DRIVEN TDD approach:

  Step 1: "I'll create `test_calculator.py` and write a test that describes what a user should experience when using the calculator..."

  ```python
  def test_user_can_add_two_numbers_and_get_correct_sum():
      # Given: User has a calculator and wants to add 2 + 3
      calculator = Calculator()
      
      # When: User adds two numbers
      result = calculator.add(2, 3)
      
      # Then: User should get the correct sum
      assert result == 5
  ```

  [Wait for user approval]
  Step 2: "Now I'll create `calculator.py` with minimal code to make this user behavior work..."

  ```python
  class Calculator:
      def add(self, a, b):
          return a + b
  ```

  [Wait for user approval]
  Step 3: "Let's add more user scenarios - what happens with negative numbers, decimals, etc..."

  output: "Following behavior-driven TDD with user-focused scenarios"

- input: |
  User modifies: "calculator/math.py"

  # Auto-triggered response:

  "File change detected in Python source. Running all tests..."
  "Running pytest..."
  "✅ All tests pass. Safe to continue development."
  OR
  "❌ 2 tests failed. Must fix before proceeding."

  output: "Continuous testing ensures code quality"

metadata:
priority: high
version: 1.0
enforcement: strict
</rule>
